{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67oiyj8IMW4u",
        "outputId": "5a62756f-80a0-4ede-dbd7-d0854c33362e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "import random\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3w_SPPONNut"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "def extract_basic_features(images):\n",
        "    return images.mean(axis=(1, 2))\n",
        "\n",
        "X_train_basic = extract_basic_features(X_train)\n",
        "X_val_basic = extract_basic_features(X_val)\n",
        "X_test_basic = extract_basic_features(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xrVkjokOeWF",
        "outputId": "7537f765-822a-46c4-92a4-31ada72fa083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Validation Accuracy with Calibration: 0.21\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Updated Decision Tree with Enhanced Depth and Calibration\n",
        "base_decision_tree = DecisionTreeClassifier(max_depth=15, min_samples_split=5, random_state=42)\n",
        "\n",
        "decision_tree = CalibratedClassifierCV(estimator=base_decision_tree, method='sigmoid', cv=5)\n",
        "decision_tree.fit(X_train_basic, y_train)\n",
        "\n",
        "val_probs_tree = decision_tree.predict_proba(X_val_basic)\n",
        "val_preds_tree = decision_tree.predict(X_val_basic)\n",
        "\n",
        "val_accuracy_tree = accuracy_score(y_val, val_preds_tree)\n",
        "print(f\"Decision Tree Validation Accuracy with Calibration: {val_accuracy_tree:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvdF0rTvOt5b",
        "outputId": "b1a42f1f-cbcc-4615-d3c4-40816731c1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1161s\u001b[0m 2s/step - accuracy: 0.2567 - loss: 2.4066 - val_accuracy: 0.4660 - val_loss: 1.4986\n",
            "Epoch 2/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 2s/step - accuracy: 0.4315 - loss: 1.5688 - val_accuracy: 0.5548 - val_loss: 1.2047\n",
            "Epoch 3/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1148s\u001b[0m 2s/step - accuracy: 0.5049 - loss: 1.3735 - val_accuracy: 0.5486 - val_loss: 1.2972\n",
            "Epoch 4/15\n",
            "\u001b[1m 81/704\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:31\u001b[0m 2s/step - accuracy: 0.5687 - loss: 1.2198"
          ]
        }
      ],
      "source": [
        "# Improved CNN Model with Additional Layers and Data Augmentation\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(data_gen.flow(X_train, y_train, batch_size=64), epochs=15, validation_data=(X_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja4iZjWRMjaK"
      },
      "outputs": [],
      "source": [
        "def hybrid_predict_optimized(X, basic_features, decision_tree, cnn_model, batch_size=32, confidence_threshold=0.7):\n",
        "    decision_tree_probs = decision_tree.predict_proba(basic_features)\n",
        "    decision_tree_preds = np.argmax(decision_tree_probs, axis=1)\n",
        "    decision_tree_confidences = np.max(decision_tree_probs, axis=1)\n",
        "\n",
        "    hybrid_preds = []\n",
        "    num_samples = len(X)\n",
        "\n",
        "\n",
        "    cnn_preds_for_uncertain = []\n",
        "\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "\n",
        "\n",
        "        batch_decision_tree_preds = decision_tree_preds[start:end]\n",
        "        batch_decision_tree_confidences = decision_tree_confidences[start:end]\n",
        "\n",
        "        uncertain_indices = np.where(batch_decision_tree_confidences < confidence_threshold)[0]\n",
        "\n",
        "\n",
        "        uncertain_batch = X[start:end][uncertain_indices]\n",
        "\n",
        "        if len(uncertain_batch) > 0:\n",
        "            cnn_preds = np.argmax(cnn_model.predict(uncertain_batch), axis=1)\n",
        "            cnn_preds_for_uncertain.extend(cnn_preds)\n",
        "\n",
        "        for i in range(len(batch_decision_tree_preds)):\n",
        "            if batch_decision_tree_confidences[i] >= confidence_threshold:\n",
        "                hybrid_preds.append(batch_decision_tree_preds[i])\n",
        "            else:\n",
        "                hybrid_preds.append(cnn_preds_for_uncertain.pop(0))\n",
        "\n",
        "    return np.array(hybrid_preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw5_ybBFMrD-"
      },
      "outputs": [],
      "source": [
        "def test_hybrid_model_on_subset(X, basic_features, decision_tree, cnn_model, sample_size=200, batch_size=32, confidence_threshold=0.7):\n",
        "    indices = random.sample(range(len(X)), sample_size)\n",
        "    X_subset = X[indices]\n",
        "    basic_features_subset = basic_features[indices]\n",
        "\n",
        "    decision_tree_probs = decision_tree.predict_proba(basic_features_subset)\n",
        "    decision_tree_confidences = np.max(decision_tree_probs, axis=1)\n",
        "\n",
        "    cnn_predictions_count = np.sum(decision_tree_confidences < confidence_threshold)\n",
        "    predictions = hybrid_predict_optimized(X_subset, basic_features_subset, decision_tree, cnn_model, batch_size=batch_size, confidence_threshold=confidence_threshold)\n",
        "\n",
        "    return predictions, indices, cnn_predictions_count\n",
        "\n",
        "def cnn_predict(X, cnn_model, batch_size=32):\n",
        "    num_samples = len(X)\n",
        "    cnn_preds = []\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "        batch = X[start:end]\n",
        "        batch_preds = np.argmax(cnn_model.predict(batch), axis=1)\n",
        "        cnn_preds.extend(batch_preds)\n",
        "    return np.array(cnn_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO1Ia1CzOuHs",
        "outputId": "f77d63d8-eaec-437a-ace3-163a660f0e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Hybrid Model - Prediction Time: 3.1171 seconds, Accuracy: 0.7317\n",
            "CNN Model - Prediction Time: 2.9926 seconds, Accuracy: 0.7333\n",
            "Overload Reduction on CNN: 1 predictions, 0.17% reduction\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sample_size = 600\n",
        "batch_size = 100\n",
        "confidence_threshold = 0.6\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "hybrid_predictions, test_indices, cnn_predictions_count = test_hybrid_model_on_subset(X_test, X_test_basic, decision_tree, cnn_model, sample_size=sample_size, batch_size=batch_size, confidence_threshold=confidence_threshold)\n",
        "hybrid_time = time.time() - start_time\n",
        "\n",
        "\n",
        "X_test_subset = X_test[test_indices]\n",
        "start_time = time.time()\n",
        "cnn_predictions = cnn_predict(X_test_subset, cnn_model, batch_size=batch_size)\n",
        "cnn_time = time.time() - start_time\n",
        "\n",
        "\n",
        "y_test_subset = y_test[test_indices]\n",
        "hybrid_accuracy = accuracy_score(y_test_subset, hybrid_predictions)\n",
        "cnn_accuracy = accuracy_score(y_test_subset, cnn_predictions)\n",
        "\n",
        "\n",
        "total_predictions_in_subset = len(X_test_subset)\n",
        "overload_reduction = total_predictions_in_subset - cnn_predictions_count\n",
        "overload_reduction_percentage = (overload_reduction / total_predictions_in_subset) * 100\n",
        "\n",
        "\n",
        "print(f\"Hybrid Model - Prediction Time: {hybrid_time:.4f} seconds, Accuracy: {hybrid_accuracy:.4f}\")\n",
        "print(f\"CNN Model - Prediction Time: {cnn_time:.4f} seconds, Accuracy: {cnn_accuracy:.4f}\")\n",
        "print(f\"Overload Reduction on CNN: {overload_reduction} predictions, {overload_reduction_percentage:.2f}% reduction\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}